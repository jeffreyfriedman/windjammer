# Windjammer Concurrency Architecture

## Overview: Three Layers of Concurrency

Windjammer provides **three complementary concurrency APIs**, each serving different use cases:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONCURRENCY LAYERS                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  1. std::compute   - Data parallelism (map/reduce)          â”‚
â”‚  2. std::thread    - Task parallelism (explicit threads)    â”‚
â”‚  3. std::async     - Async I/O (futures/promises)           â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. `std::compute` - Data Parallelism

**Purpose**: Parallel processing of collections

**Use Case**: When you have a **collection of data** and want to process each item in parallel.

**Pattern**: **Data parallelism** - same operation on multiple data items

### Example
```windjammer
use std::compute::*

// Process 1 million items in parallel
let results = parallel(items, |item| {
    // CPU-intensive operation
    process(item)
})

// Map-reduce pattern
let sum = map_reduce(data, |x| x * x, |a, b| a + b, 0)
```

### Implementation
- **Native**: `rayon::par_iter()` - work-stealing thread pool
- **WASM**: Web Workers (or sequential fallback)

### Best For
- Image processing (parallel pixel operations)
- Data transformations (map/filter/reduce)
- Batch computations
- Scientific computing

---

## 2. `std::thread` - Task Parallelism

**Purpose**: Explicit thread management

**Use Case**: When you have **independent tasks** that need to run concurrently.

**Pattern**: **Task parallelism** - different operations running simultaneously

### Example
```windjammer
use std::thread::*

// Spawn explicit threads for different tasks
let handle1 = spawn(|| {
    download_file("url1")
})

let handle2 = spawn(|| {
    process_database()
})

let result1 = handle1.join()
let result2 = handle2.join()
```

### Implementation
- **Native**: `std::thread::spawn()` - OS threads
- **WASM**: Web Workers (or error message)

### Best For
- Long-running background tasks
- Concurrent services (web server handling requests)
- Pipeline architectures
- Explicit control over thread lifecycle

---

## 3. `std::async` - Async I/O

**Purpose**: Non-blocking I/O operations

**Use Case**: When you have **I/O-bound operations** that spend time waiting.

**Pattern**: **Async/await** - cooperative multitasking

### Example
```windjammer
use std::async::*

// Async functions automatically yield during I/O
async fn fetch_data() -> string {
    let response = http::get("https://api.example.com").await
    response.body
}

// Run multiple async operations concurrently
let (data1, data2) = join!(
    fetch_data_from_api1(),
    fetch_data_from_api2()
)
```

### Implementation
- **Native**: `tokio` runtime - async executor
- **WASM**: JavaScript Promises via `wasm-bindgen-futures`

### Best For
- HTTP requests
- Database queries
- File I/O
- Network operations
- Any operation that "waits"

---

## Comparison Table

| Feature | `std::compute` | `std::thread` | `std::async` |
|---------|----------------|---------------|--------------|
| **Pattern** | Data parallelism | Task parallelism | Async I/O |
| **Use Case** | Process collections | Independent tasks | I/O operations |
| **Overhead** | Low (thread pool) | Medium (OS threads) | Very low (no threads) |
| **Scalability** | CPU cores | Limited by OS | Thousands of tasks |
| **Native Impl** | rayon | std::thread | tokio |
| **WASM Impl** | Web Workers | Web Workers | JS Promises |
| **Best For** | CPU-bound | Mixed workloads | I/O-bound |

---

## When to Use What?

### Use `std::compute` when:
âœ… Processing a **collection** of items  
âœ… Each item can be processed **independently**  
âœ… Operation is **CPU-intensive**  
âœ… You want **automatic parallelism**

**Example**: Image filters, data analysis, batch processing

```windjammer
// Perfect for compute
let processed = parallel(images, |img| apply_filter(img))
```

### Use `std::thread` when:
âœ… You have **distinct tasks** to run  
âœ… Tasks are **long-running**  
âœ… You need **explicit control** over threads  
âœ… Tasks have **different lifetimes**

**Example**: Background services, concurrent servers

```windjammer
// Perfect for threads
let server = thread::spawn(|| run_web_server())
let worker = thread::spawn(|| process_queue())
```

### Use `std::async` when:
âœ… Operations involve **I/O** (network, disk, database)  
âœ… You have **many concurrent operations**  
âœ… Operations spend time **waiting**  
âœ… You want **efficient resource usage**

**Example**: HTTP APIs, database queries, file operations

```windjammer
// Perfect for async
async fn handle_request() {
    let user = db::get_user(id).await
    let posts = db::get_posts(user.id).await
    http::json(posts)
}
```

---

## Combining the Three

You can (and should!) use all three together:

```windjammer
use std::compute::*
use std::thread::*
use std::async::*

// Async function that uses compute for CPU work
async fn process_images(urls: Vec<string>) -> Vec<Image> {
    // 1. Async: Download images concurrently
    let downloads = urls.map(|url| async {
        http::get(url).await
    })
    let raw_images = join_all(downloads).await
    
    // 2. Compute: Process images in parallel (CPU-bound)
    let processed = parallel(raw_images, |img| {
        apply_filters(img)
    })
    
    // 3. Thread: Save to disk in background
    thread::spawn(move || {
        for img in processed {
            fs::write_file(img.path, img.data)
        }
    })
    
    processed
}
```

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User's Windjammer Code                    â”‚
â”‚                                                               â”‚
â”‚  use std::compute::*  // Data parallelism                    â”‚
â”‚  use std::thread::*   // Task parallelism                    â”‚
â”‚  use std::async::*    // Async I/O                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ Compiler generates
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Platform Layer                            â”‚
â”‚                                                               â”‚
â”‚  Native:                    WASM:                            â”‚
â”‚  â”œâ”€ compute â†’ rayon         â”œâ”€ compute â†’ Web Workers        â”‚
â”‚  â”œâ”€ thread â†’ std::thread    â”œâ”€ thread â†’ Web Workers         â”‚
â”‚  â””â”€ async â†’ tokio           â””â”€ async â†’ JS Promises          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Real-World Example: Image Processing Service

```windjammer
use std::compute::*
use std::thread::*
use std::async::*

// Web server handling image processing requests
async fn main() {
    let server = http::Server::new("0.0.0.0:8080")
    
    server.post("/process", async |req| {
        // 1. ASYNC: Parse request and fetch images
        let urls = req.json::<Vec<string>>()
        let images = fetch_images(urls).await  // Concurrent downloads
        
        // 2. COMPUTE: Apply filters in parallel
        let processed = parallel(images, |img| {
            img.resize(800, 600)
               .apply_filter("sepia")
               .compress()
        })
        
        // 3. THREAD: Upload to S3 in background
        thread::spawn(move || {
            for img in processed {
                s3::upload(img)
            }
        })
        
        http::json({"status": "processing"})
    })
    
    server.run().await
}

// Async for I/O-bound downloads
async fn fetch_images(urls: Vec<string>) -> Vec<Image> {
    let futures = urls.map(|url| async {
        http::get(url).await
    })
    join_all(futures).await
}
```

---

## Implementation Status

### âœ… Fully Implemented
- `std::async` - tokio (native), JS Promises (WASM)
- `std::thread` - OS threads (native), Web Workers (WASM)
- `std::compute` - rayon (native), sequential fallback (WASM)

### ğŸš§ In Progress
- `std::compute` - Full Web Worker parallelism (WASM)

### ğŸ“‹ Planned
- `std::compute::background()` - Better integration with async
- `std::compute::stream()` - Parallel stream processing
- `std::compute::pipeline()` - Multi-stage parallel pipelines

---

## Performance Characteristics

### Overhead Comparison

```
Operation           | Overhead    | Scalability | Use Case
--------------------|-------------|-------------|------------------
async task          | ~100 bytes  | Millions    | I/O operations
compute parallel    | ~1KB/thread | CPU cores   | Data processing
thread spawn        | ~2MB/thread | Hundreds    | Long-running tasks
```

### Throughput Comparison (1M operations)

```
Scenario: Process 1 million items

Sequential:         60 seconds
std::compute:       8 seconds   (8 cores)
std::thread (8):    10 seconds  (overhead)
std::async:         N/A         (not for CPU work)
```

---

## Best Practices

### 1. Start with `std::compute` for data processing
```windjammer
// Good: Simple and automatic
let results = parallel(items, |x| process(x))

// Avoid: Manual thread management
let mut handles = vec![]
for chunk in items.chunks(100) {
    handles.push(thread::spawn(move || {
        chunk.map(|x| process(x))
    }))
}
```

### 2. Use `std::async` for I/O
```windjammer
// Good: Efficient async I/O
async fn fetch_all(urls: Vec<string>) {
    join_all(urls.map(|url| http::get(url))).await
}

// Avoid: Blocking I/O in threads
urls.map(|url| {
    thread::spawn(move || http::get_blocking(url))
})
```

### 3. Use `std::thread` for long-running services
```windjammer
// Good: Explicit thread for service
let server = thread::spawn(|| {
    run_web_server()
})

// Avoid: compute for non-data work
// (compute is for processing collections)
```

---

## Integration with Backend Proxy (Phase 2)

When we add backend proxy support, the architecture becomes:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Code: std::compute::parallel(items, fn)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                   â”‚
                    â–¼                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Native           â”‚  â”‚   WASM            â”‚
        â”‚  rayon            â”‚  â”‚   Web Workers     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   (if available)  â”‚
                               â”‚   OR               â”‚
                               â”‚   Backend Proxy   â”‚
                               â”‚   (if configured) â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚  Backend Server   â”‚
                               â”‚  (rayon)          â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This means:
1. **Native**: Direct rayon parallelism
2. **WASM (no backend)**: Sequential fallback
3. **WASM (with backend)**: Proxy to backend server that uses rayon!

---

## Summary

### Three Complementary APIs

1. **`std::compute`** - NEW! Data parallelism for collections
   - Automatic parallelism
   - CPU-bound work
   - Map/reduce patterns

2. **`std::thread`** - Existing. Task parallelism for explicit control
   - Manual thread management
   - Long-running tasks
   - Different operations

3. **`std::async`** - Existing. Async I/O for efficient waiting
   - Non-blocking I/O
   - Thousands of concurrent operations
   - Network/database/file operations

### Key Insight

These three APIs are **not competing** - they're **complementary**:
- Use `compute` for **data** (collections)
- Use `thread` for **tasks** (services)
- Use `async` for **I/O** (waiting)

And you can **mix them freely** in the same application!

---

## Next Steps

With Phase 2 (Backend Proxy), we'll add:
- Automatic backend proxying for `std::compute` in WASM
- True parallel computation in browser (via backend)
- Seamless fallback chain: Web Workers â†’ Backend â†’ Sequential

This makes Windjammer the **only language** with truly transparent concurrency across all platforms! ğŸš€

